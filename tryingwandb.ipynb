{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3ef1abf27de4a8abed82373a210904e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011111111111111112, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:01<00:00, 5.15MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 151kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.26MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 8.20MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "[Epoch 1, Batch 100] Loss: 1.0356971633434295\n",
      "[Epoch 1, Batch 200] Loss: 0.42296799302101135\n",
      "[Epoch 1, Batch 300] Loss: 0.3749110445380211\n",
      "[Epoch 1, Batch 400] Loss: 0.3723298877477646\n",
      "[Epoch 1, Batch 500] Loss: 0.3384709666669369\n",
      "[Epoch 1, Batch 600] Loss: 0.30464345283806327\n",
      "[Epoch 1, Batch 700] Loss: 0.30988201215863226\n",
      "[Epoch 1, Batch 800] Loss: 0.26651987075805667\n",
      "[Epoch 1, Batch 900] Loss: 0.26269188314676284\n",
      "[Epoch 2, Batch 100] Loss: 0.25308925986289976\n",
      "[Epoch 2, Batch 200] Loss: 0.2130228051543236\n",
      "[Epoch 2, Batch 300] Loss: 0.21778864972293377\n",
      "[Epoch 2, Batch 400] Loss: 0.19754103414714336\n",
      "[Epoch 2, Batch 500] Loss: 0.186408255584538\n",
      "[Epoch 2, Batch 600] Loss: 0.16243046775460243\n",
      "[Epoch 2, Batch 700] Loss: 0.1800655871257186\n",
      "[Epoch 2, Batch 800] Loss: 0.1673584782332182\n",
      "[Epoch 2, Batch 900] Loss: 0.17949155814945697\n",
      "[Epoch 3, Batch 100] Loss: 0.13708874419331551\n",
      "[Epoch 3, Batch 200] Loss: 0.1386475768685341\n",
      "[Epoch 3, Batch 300] Loss: 0.14605633191764356\n",
      "[Epoch 3, Batch 400] Loss: 0.15430632159113883\n",
      "[Epoch 3, Batch 500] Loss: 0.13760349206626415\n",
      "[Epoch 3, Batch 600] Loss: 0.13755154907703399\n",
      "[Epoch 3, Batch 700] Loss: 0.12319113956764341\n",
      "[Epoch 3, Batch 800] Loss: 0.14171767523512244\n",
      "[Epoch 3, Batch 900] Loss: 0.12564953546971083\n",
      "[Epoch 4, Batch 100] Loss: 0.11729526231065393\n",
      "[Epoch 4, Batch 200] Loss: 0.1024623492732644\n",
      "[Epoch 4, Batch 300] Loss: 0.10662033481523395\n",
      "[Epoch 4, Batch 400] Loss: 0.11924092929810286\n",
      "[Epoch 4, Batch 500] Loss: 0.10799423823133111\n",
      "[Epoch 4, Batch 600] Loss: 0.11428174568340183\n",
      "[Epoch 4, Batch 700] Loss: 0.10001151844859123\n",
      "[Epoch 4, Batch 800] Loss: 0.11293432336300611\n",
      "[Epoch 4, Batch 900] Loss: 0.11337631027214229\n",
      "[Epoch 5, Batch 100] Loss: 0.09583490430377424\n",
      "[Epoch 5, Batch 200] Loss: 0.10968517784029246\n",
      "[Epoch 5, Batch 300] Loss: 0.07904356906190514\n",
      "[Epoch 5, Batch 400] Loss: 0.08498561910353601\n",
      "[Epoch 5, Batch 500] Loss: 0.08805766805075109\n",
      "[Epoch 5, Batch 600] Loss: 0.1059963455516845\n",
      "[Epoch 5, Batch 700] Loss: 0.08856150551699102\n",
      "[Epoch 5, Batch 800] Loss: 0.08417048036586493\n",
      "[Epoch 5, Batch 900] Loss: 0.10887367818504572\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import wandb\n",
    "import os\n",
    "os.environ[\"WANDB_API_KEY\"] =\"61e79ad90458f5faea9605e7805f1f26268f0483\"\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "# Initialize a new wandb run\n",
    "wandb.init(project=\"simple_mnist_example\")\n",
    "\n",
    "# Define a simple neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)  # Flatten the input\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Define the model, loss function, and optimizer\n",
    "model = SimpleNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Load the MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):  # Run for 5 epochs\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader, 0):\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics and log them to wandb\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # Every 100 mini-batches\n",
    "            print(f\"[Epoch {epoch+1}, Batch {i+1}] Loss: {running_loss / 100}\")\n",
    "            \n",
    "            # Log the loss to wandb\n",
    "            wandb.log({\"Loss\": running_loss / 100})\n",
    "            \n",
    "            running_loss = 0.0\n",
    "\n",
    "# Finish the wandb run\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing input of length 10 to maximize output logits for \" world\"\n",
      "Epoch 10/100, Loss: 5.7270\n",
      "Epoch 20/100, Loss: 4.9347\n",
      "Epoch 30/100, Loss: 4.6405\n",
      "Epoch 40/100, Loss: 4.5578\n",
      "Epoch 50/100, Loss: 4.5343\n",
      "Epoch 60/100, Loss: 4.5267\n",
      "\n",
      "Optimization interrupted by user\n"
     ]
    }
   ],
   "source": [
    "# Required imports\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import wandb\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "def load_model(model_name='gpt2', device='cuda'):\n",
    "    \"\"\"\n",
    "    Load the model, tokenizer and word embeddings with proper configuration\n",
    "    \"\"\"\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Set proper padding configuration\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "    \n",
    "    word_embeddings = model.transformer.wte.weight\n",
    "    return model, word_embeddings, tokenizer\n",
    "\n",
    "def closest_tokens(embedding, word_embeddings, tokenizer, n=1):\n",
    "    \"\"\"\n",
    "    Find the closest tokens to a given embedding\n",
    "    \"\"\"\n",
    "    with torch.no_grad():  # No need to track gradients here\n",
    "        distances = torch.sqrt(((word_embeddings - embedding.unsqueeze(0))**2).sum(-1))\n",
    "        closest_indices = distances.argsort()[:n]\n",
    "        closest_words = [tokenizer.decode([idx.item()]) for idx in closest_indices]\n",
    "        return closest_words, closest_indices, distances[closest_indices], embedding\n",
    "\n",
    "def model_emb(model, input_embeddings, word_embeddings, output_len):\n",
    "    \"\"\"\n",
    "    Get model outputs for given input embeddings\n",
    "    \"\"\"\n",
    "    # Create attention mask (all 1s since we're not padding)\n",
    "    attention_mask = torch.ones(\n",
    "        (input_embeddings.shape[0], input_embeddings.shape[1]), \n",
    "        device=input_embeddings.device\n",
    "    )\n",
    "    \n",
    "    outputs = model(\n",
    "        inputs_embeds=input_embeddings,\n",
    "        attention_mask=attention_mask\n",
    "    )\n",
    "    \n",
    "    logits = outputs.logits[:, :output_len, :]\n",
    "    \n",
    "    # Calculate perplexity\n",
    "    with torch.no_grad():\n",
    "        log_probs = torch.log_softmax(logits, dim=-1)\n",
    "        perplexity = -log_probs.mean(dim=-1)\n",
    "    \n",
    "    return logits, input_embeddings, perplexity\n",
    "\n",
    "def optimize_input(\n",
    "    model,\n",
    "    word_embeddings,\n",
    "    tokenizer,\n",
    "    device,\n",
    "    input_len=10,\n",
    "    target_output=' world',\n",
    "    batch_size=20,\n",
    "    epochs=100,\n",
    "    lr=0.1,\n",
    "    dist_reg=0.1,\n",
    "    verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Optimize input embeddings to generate desired target output\n",
    "    \"\"\"\n",
    "    print(f'Optimizing input of length {input_len} to maximize output logits for \"{target_output}\"')\n",
    "    \n",
    "    # Get target output indices\n",
    "    output_ix = tokenizer.encode(target_output, return_tensors='pt')[0].to(device)\n",
    "    output_len = output_ix.shape[0]\n",
    "    \n",
    "    # Normalize word embeddings\n",
    "    word_embeddings = word_embeddings / torch.sqrt(torch.sum(word_embeddings**2, dim=-1, keepdim=True))\n",
    "    \n",
    "    # Initialize input embeddings\n",
    "    start_input = word_embeddings[torch.randperm(word_embeddings.shape[0])[:input_len * batch_size]].reshape(\n",
    "        batch_size, input_len, -1)\n",
    "    input_embeds = torch.nn.Parameter(start_input.to(device), requires_grad=True)\n",
    "    \n",
    "    # Setup optimizer\n",
    "    optimizer = torch.optim.Adam([input_embeds], lr=lr, eps=0.0001)\n",
    "    \n",
    "    optimized_inputs = set()\n",
    "    \n",
    "    try:\n",
    "        for epoch in range(epochs):\n",
    "            # Normalize input embeddings\n",
    "            norm_input = input_embeds / torch.sqrt(torch.sum(input_embeds**2, dim=-1, keepdim=True))\n",
    "            \n",
    "            # Get model outputs\n",
    "            logits, emb, perp = model_emb(model, norm_input, word_embeddings, output_len)\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            \n",
    "            # Calculate target probabilities\n",
    "            target_probs = probs[:, torch.arange(output_len), output_ix]\n",
    "            \n",
    "            # Calculate token distances\n",
    "            token_distances = []\n",
    "            closest_indices = []\n",
    "            \n",
    "            for batch in norm_input:\n",
    "                batch_distances = []\n",
    "                batch_indices = []\n",
    "                for embedding in batch:\n",
    "                    _, idx, dist, _ = closest_tokens(embedding, word_embeddings, tokenizer)\n",
    "                    batch_distances.append(dist)\n",
    "                    batch_indices.append(idx)\n",
    "                token_distances.append(torch.stack(batch_distances))\n",
    "                closest_indices.append(torch.stack(batch_indices))\n",
    "            \n",
    "            token_distances = torch.stack(token_distances).squeeze(-1)\n",
    "            closest_indices = torch.stack(closest_indices).squeeze(-1)\n",
    "            \n",
    "            # Calculate losses\n",
    "            prob_loss = -torch.log(target_probs).mean()\n",
    "            dist_loss = token_distances.mean() * dist_reg\n",
    "            total_loss = prob_loss + dist_loss\n",
    "            \n",
    "            # Generate outputs using closest tokens\n",
    "            with torch.no_grad():\n",
    "                attention_mask = torch.ones((batch_size, input_len), device=device)\n",
    "                model_outputs = model.generate(\n",
    "                    closest_indices,\n",
    "                    max_length=output_len + input_len,\n",
    "                    pad_token_id=model.config.pad_token_id,\n",
    "                    attention_mask=attention_mask\n",
    "                )\n",
    "            \n",
    "            # Check for successful generations\n",
    "            for batch_idx in range(batch_size):\n",
    "                output_text = tokenizer.decode(model_outputs[batch_idx][input_len:])\n",
    "                if target_output in output_text:\n",
    "                    full_text = tokenizer.decode(model_outputs[batch_idx])\n",
    "                    if full_text not in optimized_inputs:\n",
    "                        optimized_inputs.add(full_text)\n",
    "                        if verbose:\n",
    "                            print(f\"\\nFound new solution (Epoch {epoch}):\")\n",
    "                            print(f\"Input: {tokenizer.decode(model_outputs[batch_idx][:input_len])}\")\n",
    "                            print(f\"Output: {output_text}\")\n",
    "            \n",
    "            # Optimization step\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Log progress\n",
    "            if verbose and (epoch + 1) % 10 == 0:\n",
    "                print(f'Epoch {epoch+1}/{epochs}, Loss: {total_loss.item():.4f}')\n",
    "                \n",
    "            # Optional: Log to wandb\n",
    "            wandb.log({\n",
    "                'total_loss': total_loss.item(),\n",
    "                'prob_loss': prob_loss.item(),\n",
    "                'dist_loss': dist_loss.item(),\n",
    "                'num_solutions': len(optimized_inputs)\n",
    "            })\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nOptimization interrupted by user\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        raise\n",
    "    \n",
    "    return optimized_inputs\n",
    "\n",
    "# Example usage in notebook cells:\n",
    "\n",
    "# Cell 1: Initialize wandb (optional)\n",
    "def init_wandb(run_name=\"test-run\"):\n",
    "    try:\n",
    "        wandb.init(project='model-optimization', name=run_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not initialize wandb: {str(e)}\")\n",
    "        print(\"Continuing without wandb logging...\")\n",
    "\n",
    "# Cell 2: Load model and dependencies\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model, word_embeddings, tokenizer = load_model(device=device)\n",
    "\n",
    "# Cell 3: Run optimization\n",
    "init_wandb()  # Comment this out if you don't want to use wandb\n",
    "results = optimize_input(\n",
    "    model=model,\n",
    "    word_embeddings=word_embeddings,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    input_len=10,\n",
    "    target_output=' world',\n",
    "    batch_size=20,\n",
    "    epochs=100,\n",
    "    lr=0.01,\n",
    "    dist_reg=0.1,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "def optimize_input(\n",
    "    model,\n",
    "    word_embeddings,\n",
    "    tokenizer,\n",
    "    device,\n",
    "    input_len=10,\n",
    "    target_output=' world',\n",
    "    batch_size=20,\n",
    "    epochs=100,\n",
    "    lr=0.01,\n",
    "    dist_reg=0.05,\n",
    "    verbose=True\n",
    "):\n",
    "    print(f'Optimizing input of length {input_len} to maximize output logits for \"{target_output}\"')\n",
    "    \n",
    "    output_ix = tokenizer.encode(target_output, return_tensors='pt')[0].to(device)\n",
    "    output_len = output_ix.shape[0]\n",
    "    \n",
    "    word_embeddings = word_embeddings / torch.sqrt(torch.sum(word_embeddings**2, dim=-1, keepdim=True))\n",
    "    \n",
    "    vocab_size = word_embeddings.shape[0]\n",
    "    indices = torch.randperm(vocab_size)[:input_len * batch_size]\n",
    "    start_input = word_embeddings[indices].reshape(batch_size, input_len, -1)\n",
    "    input_embeds = torch.nn.Parameter(start_input.to(device), requires_grad=True)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW([input_embeds], lr=lr, weight_decay=0.01)\n",
    "    scheduler = torch.optim.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=True)\n",
    "    \n",
    "    optimized_inputs = set()\n",
    "    best_loss = float('inf')\n",
    "    patience = 0\n",
    "    max_patience = 20\n",
    "    \n",
    "    try:\n",
    "        for epoch in range(epochs):\n",
    "            norm_input = input_embeds / (torch.norm(input_embeds, dim=-1, keepdim=True) + 1e-6)\n",
    "            \n",
    "            logits, emb, perp = model_emb(model, norm_input, word_embeddings, output_len)\n",
    "            \n",
    "            log_probs = torch.log_softmax(logits, dim=-1)\n",
    "            target_log_probs = log_probs[:, torch.arange(output_len), output_ix]\n",
    "            \n",
    "            position_weights = torch.linspace(1.0, 0.5, output_len).to(device)\n",
    "            prob_loss = -(target_log_probs * position_weights).mean()\n",
    "            \n",
    "            token_distances = []\n",
    "            closest_indices = []\n",
    "            \n",
    "            for batch in norm_input:\n",
    "                batch_distances = []\n",
    "                batch_indices = []\n",
    "                for embedding in batch:\n",
    "                    _, idx, dist, _ = closest_tokens(embedding, word_embeddings, tokenizer)\n",
    "                    batch_distances.append(dist)\n",
    "                    batch_indices.append(idx)\n",
    "                token_distances.append(torch.stack(batch_distances))\n",
    "                closest_indices.append(torch.stack(batch_indices))\n",
    "            \n",
    "            token_distances = torch.stack(token_distances).squeeze(-1)\n",
    "            closest_indices = torch.stack(closest_indices).squeeze(-1)\n",
    "            \n",
    "            # Convert unique penalty to tensor\n",
    "            unique_tokens = torch.tensor([len(set(closest_indices[b].tolist())) for b in range(batch_size)], \n",
    "                                      device=device, dtype=torch.float)\n",
    "            unique_penalty = (1.0 - unique_tokens / input_len).mean()\n",
    "            \n",
    "            dist_loss = token_distances.mean() * dist_reg\n",
    "            diversity_loss = unique_penalty * 0.1\n",
    "            total_loss = prob_loss + dist_loss + diversity_loss\n",
    "            \n",
    "            if total_loss.item() < best_loss:\n",
    "                best_loss = total_loss.item()\n",
    "                patience = 0\n",
    "            else:\n",
    "                patience += 1\n",
    "            \n",
    "            if patience >= max_patience:\n",
    "                print(f\"\\nEarly stopping at epoch {epoch} due to no improvement\")\n",
    "                break\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                attention_mask = torch.ones((batch_size, input_len), device=device)\n",
    "                model_outputs = model.generate(\n",
    "                    closest_indices,\n",
    "                    max_length=output_len + input_len,\n",
    "                    pad_token_id=model.config.pad_token_id,\n",
    "                    attention_mask=attention_mask,\n",
    "                    do_sample=True,\n",
    "                    top_p=0.9,\n",
    "                    temperature=0.7\n",
    "                )\n",
    "            \n",
    "            for batch_idx in range(batch_size):\n",
    "                output_text = tokenizer.decode(model_outputs[batch_idx][input_len:])\n",
    "                if target_output in output_text:\n",
    "                    full_text = tokenizer.decode(model_outputs[batch_idx])\n",
    "                    if full_text not in optimized_inputs:\n",
    "                        optimized_inputs.add(full_text)\n",
    "                        if verbose:\n",
    "                            print(f\"\\nFound new solution (Epoch {epoch}):\")\n",
    "                            print(f\"Input: {tokenizer.decode(model_outputs[batch_idx][:input_len])}\")\n",
    "                            print(f\"Output: {output_text}\")\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_([input_embeds], max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step(total_loss)\n",
    "            \n",
    "            if verbose and (epoch + 1) % 10 == 0:\n",
    "                print(f'Epoch {epoch+1}/{epochs}, Loss: {total_loss.item():.4f} '\n",
    "                      f'(Prob: {prob_loss.item():.4f}, Dist: {dist_loss.item():.4f}, '\n",
    "                      f'Div: {diversity_loss.item():.4f})')\n",
    "                \n",
    "            wandb.log({\n",
    "                'total_loss': total_loss.item(),\n",
    "                'prob_loss': prob_loss.item(),\n",
    "                'dist_loss': dist_loss.item(),\n",
    "                'diversity_loss': diversity_loss.item(),\n",
    "                'num_solutions': len(optimized_inputs),\n",
    "                'learning_rate': optimizer.param_groups[0]['lr']\n",
    "            })\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nOptimization interrupted by user\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        raise\n",
    "    \n",
    "    return optimized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing input of length 10 to maximize output logits for \" world\"\n",
      "Epoch 10/100, Loss: 5.6669 (Prob: 5.6607, Dist: 0.0061, Div: 0.0000)\n",
      "Epoch 20/100, Loss: 4.8991 (Prob: 4.8925, Dist: 0.0066, Div: 0.0000)\n",
      "Epoch 30/100, Loss: 4.6261 (Prob: 4.6196, Dist: 0.0065, Div: 0.0000)\n",
      "Epoch 40/100, Loss: 4.5486 (Prob: 4.5421, Dist: 0.0065, Div: 0.0000)\n",
      "Epoch 50/100, Loss: 4.5271 (Prob: 4.5206, Dist: 0.0065, Div: 0.0000)\n",
      "Epoch 60/100, Loss: 4.5200 (Prob: 4.5135, Dist: 0.0065, Div: 0.0000)\n",
      "Epoch 70/100, Loss: 4.5176 (Prob: 4.5111, Dist: 0.0065, Div: 0.0000)\n",
      "Epoch 80/100, Loss: 4.5167 (Prob: 4.5102, Dist: 0.0065, Div: 0.0000)\n",
      "Epoch 90/100, Loss: 4.5164 (Prob: 4.5099, Dist: 0.0065, Div: 0.0000)\n",
      "Epoch 100/100, Loss: 4.5164 (Prob: 4.5098, Dist: 0.0065, Div: 0.0000)\n"
     ]
    }
   ],
   "source": [
    "results = optimize_input(\n",
    "    model=model,\n",
    "    word_embeddings=word_embeddings,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    input_len=10,\n",
    "    target_output=' world',\n",
    "    batch_size=20,\n",
    "    epochs=100,\n",
    "    lr=0.01,  # Using lower learning rate\n",
    "    dist_reg=0.05,  # Lower distance regularization\n",
    "    verbose=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
